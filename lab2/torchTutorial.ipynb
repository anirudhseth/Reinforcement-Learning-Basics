{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('rllab': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f91741d54cd3fef77d01d2f5661a8b1917fcda1cb9ff114fc0f21f2e9b0b96df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "### Experience class ###\n",
    "\n",
    "# namedtuple is used to create a special type of tuple object. Namedtuples\n",
    "# always have a specific name (like a class) and specific fields.\n",
    "# In this case I will create a namedtuple 'Experience',\n",
    "# with fields: state, action, reward,  next_state, done.\n",
    "# Usage: for some given variables s, a, r, s, d you can write for example\n",
    "# exp = Experience(s, a, r, s, d). Then you can access the reward\n",
    "# field by  typing exp.reward\n",
    "Experience = namedtuple('Experience',\n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ExperienceReplayBuffer(object):\n",
    "    \"\"\" Class used to store a buffer containing experiences of the RL agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_length):\n",
    "        # Create buffer of maximum length\n",
    "        self.buffer = deque(maxlen=maximum_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # Append experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        # overload len operator\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\" Function used to sample experiences from the buffer.\n",
    "            returns 5 lists, each of size n. Returns a list of state, actions,\n",
    "            rewards, next states and done variables.\n",
    "        \"\"\"\n",
    "        # If we try to sample more elements that what are available from the\n",
    "        # buffer we raise an error\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Tried to sample too many elements from the buffer!')\n",
    "\n",
    "        # Sample without replacement the indices of the experiences\n",
    "        # np.random.choice takes 3 parameters: number of elements of the buffer,\n",
    "        # number of elements to sample and replacement.\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size=n,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        # Using the indices that we just sampled build a list of chosen experiences\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        # batch is a list of size n, where each element is an Experience tuple\n",
    "        # of 5 elements. To convert a list of tuples into\n",
    "        # a tuple of list we do zip(*batch). In this case this will return a\n",
    "        # tuple of 5 elements where each element is a list of n elements.\n",
    "        return zip(*batch)\n",
    "\n",
    "### Neural Network ###\n",
    "class MyNetwork(nn.Module):\n",
    "    \"\"\" Create a feedforward neural network \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create input layer with ReLU activation\n",
    "        self.input_layer = nn.Linear(input_size, 8)\n",
    "        self.input_layer_activation = nn.ReLU()\n",
    "\n",
    "        # Create output layer\n",
    "        self.output_layer = nn.Linear(8, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Function used to compute the forward pass\n",
    "\n",
    "        # Compute first layer\n",
    "        l1 = self.input_layer(x)\n",
    "        l1 = self.input_layer_activation(l1)\n",
    "\n",
    "        # Compute output layer\n",
    "        out = self.output_layer(l1)\n",
    "        return out\n",
    "\n",
    "\n",
    "### CREATE RL ENVIRONMENT ###\n",
    "env = gym.make('CartPole-v0')        # Create a CartPole environment\n",
    "n = len(env.observation_space.low)   # State space dimensionality\n",
    "m = env.action_space.n               # Number of actions\n",
    "\n",
    "### Create Experience replay buffer ###\n",
    "buffer = ExperienceReplayBuffer(maximum_length=1000)\n",
    "\n",
    "### Create network ###\n",
    "network = MyNetwork(input_size=n, output_size=m)\n",
    "\n",
    "### Create optimizer ###\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "### PLAY ENVIRONMENT ###\n",
    "# The next while loop plays 5 episode of the environment\n",
    "for episode in range(5):\n",
    "    state = env.reset()                    # Reset environment, returns\n",
    "                                           # initial state\n",
    "    done = False                           # Boolean variable used to indicate\n",
    "                                           # if an episode terminated\n",
    "    while not done:\n",
    "        env.render()                       # Render the environment, remove this\n",
    "                                           # line if you run on Google Colab\n",
    "        # Create state tensor, remember to use single precision (torch.float32)\n",
    "        state_tensor = torch.tensor([state],\n",
    "                                    requires_grad=False,\n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        # Compute output of the network\n",
    "        values = network(state_tensor)\n",
    "\n",
    "        # Pick the action with greatest value\n",
    "        #.max(1) picks the action with maximum value along the first dimension\n",
    "        #[1] picks the argmax\n",
    "        #.item() is used to cast the tensor to a real value\n",
    "        action = values.max(1)[1].item()\n",
    "\n",
    "        # The next line takes permits you to take an action in the RL environment\n",
    "        # env.step(action) returns 4 variables:\n",
    "        # (1) next state; (2) reward; (3) done variable; (4) additional stuff\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Append experience to the buffer\n",
    "        exp = Experience(state, action, reward, next_state, done)\n",
    "        buffer.append(exp)\n",
    "\n",
    "        ### TRAINING ###\n",
    "        # Perform training only if we have more than 3 elements in the buffer\n",
    "        if len(buffer) >= 3:\n",
    "            # Sample a batch of 3 elements\n",
    "            states, actions, rewards, next_states, dones = buffer.sample_batch(\n",
    "                n=3)\n",
    "\n",
    "            # Training process, set gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute output of the network given the states batch\n",
    "            values = network(torch.tensor(states,\n",
    "                            requires_grad=True,\n",
    "                            dtype=torch.float32))\n",
    "\n",
    "            # Compute loss function\n",
    "            loss = nn.functional.mse_loss(\n",
    "                            values,\n",
    "                            torch.zeros_like(values, requires_grad=False))\n",
    "\n",
    "            # Compute gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient norm to 1\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.)\n",
    "\n",
    "            # Perform backward pass (backpropagation)\n",
    "            optimizer.step()\n",
    "\n",
    "# Close all the windows\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}